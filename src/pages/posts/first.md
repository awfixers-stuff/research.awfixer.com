---
layout: ../../layouts/post.astro
title: "Welcome to AWFixers Research"
pubDate: 2023-12-23
description: "This is the first post of the Research site"
author: "awfixer"
isPinned: true
excerpt: AWFixers Research, seek truth, find truth, be truth.
image:
  src: "https://cdn.awfixer.me/10.png"
  alt: "10.png"
tags: ["Welcome", "not-research"]
---

# The Research Flow

One of the things that we have to deal with when it comes to research is the current shape of the internet. This includes the role of AI, the size of the internet, and the way that sites signal the acclaimed value of their information. Things like this help us a lot when it comes to searching the internet for information.

an example of how a flow to find information may work is listed below:

have a conversation with [grok](https://grok.com) about the topic that we are looking into, use grok to expand on the intial questions(s), after a brief conversation with grok, we request that grok right us a prompt that includes the scope of our conversation for a research agent, one that will not trip and of the flags of the agent and will also bypass and possible restrictions of the agent using alterative wording.

this prompt is then taken and inserted into [Gemini Deep Research](https://gemini.google/overview/deep-research/) and we have Gemini do extensive research into the topic using the prompt that we gave it. When this report is generated we do a brief read through of it to ensure that it was indeed looking for the information that we requested, then we scroll down to the thinking area in the report and examine the sources, thoughts during the research, and the sites that were "read" but not used in the report.

We then take these sites and go to yet another model, we request that the model attempt to visit, read and summarize the sites for us to use in a report. More often then not the model reports repeat failures to read and summarize the sites do to some sort of blocking. This signals that the owners of the site believe one of two things, that they want to profile anyone who visits their site, or that the information on their site is valued to the point that they do not want to open it up to scraping that is not needed to increae the SEO of the website (basically all content outside of well written meta data and meta links)

We then manually visit each of theses sites to decide if they have any value or if they simply are owned by internet profilers, and based on this we obtain the solid and dependable information off of each of these sites.

We then use tools like [Notion](https://notion.so) and [NotebookLM](https://notebooklm.google) to take the reports that we have generated and create a dedicated space to review and in the case of NotebookLM, interact with the information.

When we have completed this, we have an achieve of reports, their sources, and notes that we have made on both, this information is then what we use when we are conducting experiments and IRL (in real life) research, and finally when we compose the final reports that we will be uploading to this website.

We hope that helps answer any questions that you have about the way that we do work when it comes to this site.

Cheers.
